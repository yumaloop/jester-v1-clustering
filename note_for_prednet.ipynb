{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from prednet import PredNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "data_format='channels_first'の場合は次のshapeの5階テンソル：(samples, time, channels, rows, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredNet( ):\n",
    "    def __init__(self, T=40, L=10, img_shape=(64,64,3)):\n",
    "        self.T = T\n",
    "        self.L = L\n",
    "        self.img_h = img_shape[0]\n",
    "        self.img_w = img_shape[1]\n",
    "        self.img_c = img_shape[2]\n",
    "        \n",
    "        self.x_shape = (self.T, 1, self.img_h, self.img_w, self.img_c)\n",
    "        self.e_shape = (self.T, self.L, self.img_h, self.img_w, self.img_c*2)\n",
    "        self.r_shape = (self.T, self.L, self.img_h, self.img_w, self.img_c)\n",
    "        \n",
    "    def build_train(self):\n",
    "        # Input Lauer\n",
    "        # T: 50, L: 10, h:64, w:64, c:3\n",
    "        X_input = keras.layers.Input(shape=self.x_shape, dtype='float32', name='x_input')\n",
    "        E_input = keras.layers.Input(shape=self.e_shape, dtype='float32', name='e_input')\n",
    "        R_input = keras.layers.Input(shape=self.r_shape, dtype='float32', name='r_input')\n",
    "        # print(\"L:\", self.L, \"T:\", self.T)\n",
    "\n",
    "        for t in range(self.T):\n",
    "\n",
    "            \"\"\" === E unit === \"\"\"\n",
    "            if t == 0:\n",
    "                E = E_input\n",
    "\n",
    "            def E_to_Et(E):\n",
    "                # E_t: (None, L, h, w, 2c)\n",
    "                E_t = K.permute_dimensions(E, [1, 0, 2, 3, 4, 5])\n",
    "                E_t = K.gather(E_t, [t])\n",
    "                E_t = K.permute_dimensions(E_t, [1, 0, 2, 3, 4, 5])\n",
    "                E_t = K.squeeze(E_t, axis=1)\n",
    "                return E_t\n",
    "            E_t = keras.layers.Lambda(E_to_Et)(E)\n",
    "\n",
    "            \"\"\" === R unit === \"\"\"\n",
    "            if t ==0:\n",
    "                def R_to_Rt(R_input):\n",
    "                    R_t = K.permute_dimensions(R_input, [1, 0, 2, 3, 4, 5])\n",
    "                    R_t = K.gather(R_t, [t])\n",
    "                    R_t = K.permute_dimensions(R_t, [1, 0, 2, 3, 4, 5])\n",
    "                    R_t = K.squeeze(R_t, axis=1)\n",
    "                    return R_t\n",
    "                R_t = keras.layers.Lambda(R_to_Rt)(R_input)\n",
    "            else:\n",
    "                # R_t: (None, L, h, w, 3)\n",
    "                E_t_rev = keras.layers.Lambda(lambda x: K.reverse(x, axes=1))(E_t)\n",
    "                R_t, state_h_t, state_c_t = keras.layers.ConvLSTM2D(3, (3, 3), \n",
    "                                                                    padding='same', \n",
    "                                                                    activation='tanh', \n",
    "                                                                    return_sequences=True, \n",
    "                                                                    return_state=True)(E_t_rev)\n",
    "\n",
    "            \n",
    "            for l in range(self.L):  \n",
    "                \"\"\" === R_tl === \"\"\"\n",
    "                def Rt_to_Rtl(R_t):\n",
    "                    R_tl = K.permute_dimensions(R_t, [1, 0, 2, 3, 4])\n",
    "                    R_tl = K.gather(R_tl, [l])\n",
    "                    R_tl = K.permute_dimensions(R_tl, [1, 0, 2, 3, 4])\n",
    "                    R_tl = K.squeeze(R_tl, axis=1)\n",
    "                    return R_tl\n",
    "                def Rt_to_Rtl_shape(input_shape):\n",
    "                    output_shape = (input_shape[0],) + input_shape[-3:]\n",
    "                    return output_shape\n",
    "\n",
    "                # R_tl: (None, h, w, 3)\n",
    "                R_tl = keras.layers.Lambda(Rt_to_Rtl, Rt_to_Rtl_shape)(R_t)                \n",
    "                \n",
    "                \"\"\" === Ahat_tl === \"\"\"\n",
    "                # Ahat_tl: (None, h, w, 3)\n",
    "                Ahat_tl = keras.layers.Conv2D(3, (3, 3), padding='same')(R_tl)\n",
    "                Ahat_tl = keras.layers.Activation('relu')(Ahat_tl)\n",
    "\n",
    "                if l == 0:                    \n",
    "                    def X_to_Atl(x_input):\n",
    "                        # (None, T, 1, h, w, c) --> (?, h, w, c)\n",
    "                        A_tl = K.squeeze(x_input, axis=2) # (None, T, 64, 64, 3)\n",
    "                        A_tl = K.permute_dimensions(A_tl, [1, 0, 2, 3, 4]) # (T, None, 64, 64, 3)\n",
    "                        A_tl = K.gather(A_tl, [t]) # (1, None, 64, 64, 3)\n",
    "                        A_tl = K.squeeze(A_tl, axis=0) # (None, 64, 64, 3)\n",
    "                        return A_tl\n",
    "                    def X_to_Atl_shape(input_shape):\n",
    "                        output_shape = (input_shape[0],) + input_shape[-3:]\n",
    "                        return output_shape\n",
    "\n",
    "                    # A_tl: (None, h, w, 3)\n",
    "                    A_tl = keras.layers.Lambda(X_to_Atl)(X_input)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                # E_tl: (None, h, w, 3)\n",
    "                err0_tl = keras.layers.Subtract()([A_tl, Ahat_tl])\n",
    "                err0_tl = keras.layers.Activation('relu')(err0_tl)\n",
    "                err1_tl = keras.layers.Subtract()([Ahat_tl, A_tl])\n",
    "                err1_tl = keras.layers.Activation('relu')(err1_tl)\n",
    "                E_tl =  keras.layers.Concatenate(axis=-1)([err0_tl, err1_tl])\n",
    "\n",
    "                # A_tl: (None, h, w, 3)\n",
    "                if l != self.L-1:\n",
    "                    def Et_to_Etl(E_t):\n",
    "                        E_tl = K.permute_dimensions(E_t, [1, 0, 2, 3, 4])\n",
    "                        E_tl = K.gather(E_tl, [l])\n",
    "                        E_tl = K.permute_dimensions(E_tl, [1, 0, 2, 3, 4])\n",
    "                        E_tl = K.squeeze(E_tl, axis=1)\n",
    "                        return E_tl\n",
    "\n",
    "                    # E_tl: (None, h, w, 3)\n",
    "                    E_tl = keras.layers.Lambda(Et_to_Etl)(E_t)\n",
    "                    # A_tl\n",
    "                    A_tl = keras.layers.Conv2D(3, (3,3), padding='same')(E_tl)\n",
    "\n",
    "                if l == 0:\n",
    "                    E_l = keras.layers.Lambda(lambda x: K.expand_dims(x, axis=1))(E_tl)\n",
    "                else:                \n",
    "                    E_tl_ = keras.layers.Lambda(lambda x: K.expand_dims(x, axis=1))(E_tl)\n",
    "                    E_l = keras.layers.Concatenate(axis=1)([E_l, E_tl_])\n",
    "            if t == 0:\n",
    "                E = keras.layers.Lambda(lambda x : K.expand_dims(x, axis=1))(E_l)\n",
    "            else:\n",
    "                E_l_ = keras.layers.Lambda(lambda x : K.expand_dims(x, axis=1))(E_l)\n",
    "                E = keras.layers.Concatenate(axis=1)([E, E_l_])\n",
    "\n",
    "\n",
    "        # print(E) # 50, 10, 64, 64, 6\n",
    "        model_train = keras.models.Model(inputs=[X_input, E_input, R_input], outputs=[E])\n",
    "        return model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = PredNet(T=40, L=10, img_shape=(64, 64, 3)).build_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class PredNetBatchGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, video_path=\"./data/video/20bn-jester-v1\",\n",
    "                 img_size=(64, 64), \n",
    "                 batch_size=1,\n",
    "                 max_frame_size=40,\n",
    "                 use_padding=True):\n",
    "        \n",
    "        # Load CSV as pd.DataFrame\n",
    "        self.df_labels = pd.read_csv(\"./data/labels.csv\", header=None, names=[\"jester name\"])\n",
    "        self.df_labels['label'] = self.df_labels.index.to_series()\n",
    "        self.df_train = pd.read_csv(\"./data/train.csv\", sep=\";\", header=None, names=[\"frame_id\", \"jester name\"])\n",
    "        self.df_train = pd.merge(self.df_train, self.df_labels, how=\"left\", on = \"jester name\")\n",
    "\n",
    "        # debug \n",
    "        # TODO: delete this line\n",
    "        self.df_train = self.df_train[:10000]\n",
    "        self.max_frame_size = max_frame_size\n",
    "        \n",
    "        self.video_path = video_path\n",
    "        self.num = len(self.df_train)\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.use_padding = use_padding\n",
    "        self.batches_per_epoch = int((self.num - 1) / batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx: batch id\n",
    "        \"\"\"\n",
    "        batch_from = self.batch_size * idx\n",
    "        batch_to = batch_from + self.batch_size\n",
    "\n",
    "        if batch_to > self.num:\n",
    "            batch_to = self.num\n",
    "\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for index, row in self.df_train[batch_from:batch_to].iterrows(): \n",
    "            video=[]\n",
    "            for i, img_filename in enumerate(os.listdir(os.path.join(self.video_path, str(row[\"frame_id\"])))):\n",
    "                img_path = os.path.join(self.video_path, str(row[\"frame_id\"]), str(img_filename))\n",
    "                img_pil = Image.open(img_path).resize(self.img_size)\n",
    "                img_arr = np.array(img_pil)\n",
    "                video.append(img_arr)\n",
    "            video = np.array(video)\n",
    "\n",
    "            x_batch.append(video)\n",
    "            y_batch.append(row[\"label\"])\n",
    "\n",
    "\n",
    "        # Reverce list \n",
    "        x_batch_r = deepcopy(x_batch)\n",
    "        x_batch_r.reverse()\n",
    "\n",
    "        # Zero padding\n",
    "        if self.use_padding:\n",
    "            x_batch   = self._zero_padding(x_batch, self.max_frame_size)\n",
    "            x_batch_r = self._zero_padding(x_batch_r, self.max_frame_size)\n",
    "\n",
    "        x_batch = np.asarray(x_batch)\n",
    "        x_batch = x_batch.astype('float32') / 255.0\n",
    "        x_batch_r = np.asarray(x_batch_r)\n",
    "        x_batch_r = x_batch_r.astype('float32') / 255.0\n",
    "        # y_batch = np.asarray(y_batch)\n",
    "        \n",
    "        x_batch = np.expand_dims(x_batch, axis=2)\n",
    "        z_batch = np.zeros_like(x_batch)\n",
    "        \n",
    "        e_batch = np.zeros(x_batch[:2] + (self.max_frame_size,) + x_batch[-3:])\n",
    "        r_batch = np.zeros(x_batch[:2] + (self.max_frame_size,) + x_batch[-3:])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # videos, videos\n",
    "        return [x_batch, e_batch, r_batch], [z_batch]\n",
    "\n",
    "    def _zero_padding(self, videos, max_frame_size):\n",
    "        videos_pad=[]\n",
    "        for v in videos:\n",
    "            if v.shape[0] < max_frame_size:\n",
    "                diff = max_frame_size - v.shape[0]\n",
    "                v_pad = np.pad(v, [(0,diff),(0,0),(0,0),(0,0)], 'constant')\n",
    "                videos_pad.append(v_pad)\n",
    "            else:\n",
    "                videos_pad.append(v)\n",
    "        return videos_pad\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        batch length: 1epochのバッチ数\n",
    "        \"\"\"\n",
    "        return self.batches_per_epoch\n",
    "\n",
    "    def __getlabel__(self, idx):\n",
    "        batch_from = self.batch_size * idx\n",
    "        batch_to = batch_from + self.batch_size\n",
    "\n",
    "        if batch_to > self.num:\n",
    "            batch_to = self.num\n",
    "\n",
    "        label_batch = []\n",
    "        for index, row in self.df_train[batch_from:batch_to].iterrows(): \n",
    "            label_batch.append(row[\"label\"])\n",
    "\n",
    "        return np.array(label_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # 1epochが終わった時の処理\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "model logdir : ./log/prednet_20191126 17:09:51\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-de36bfd9d7ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                   \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                                   initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(uid, i)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mat\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mi\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \"\"\"\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ba76debb0793>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mz_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0me_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_frame_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mr_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_frame_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import datetime\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "model_train = PredNet(T=40, L=10, img_shape=(64, 64, 3)).build_train()\n",
    "model_train.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "# model_train.summary()\n",
    "\n",
    "\n",
    "train_batch_generator = PredNetBatchGenerator(video_path=\"./data/video/20bn-jester-v1\",\n",
    "                                              img_size=(48, 48), \n",
    "                                              batch_size=4,\n",
    "                                              max_frame_size=40,\n",
    "                                              use_padding=True)\n",
    "\n",
    "date_string = \"prednet_\"+datetime.datetime.now().strftime('%Y%m%d %H:%M:%S')\n",
    "os.mkdir('./log/'+date_string)\n",
    "print(\"model logdir :\", \"./log/\"+date_string)\n",
    "\n",
    "callbacks=[]\n",
    "callbacks.append(keras.callbacks.CSVLogger(filename='./log/'+date_string+'/metrics.csv'))\n",
    "callbacks.append(keras.callbacks.ModelCheckpoint(filepath='./log/'+date_string+'/bestweights.hdf5', \n",
    "                                                    monitor='loss', \n",
    "                                                    save_best_only=True))\n",
    "\n",
    "history= model_train.fit_generator(train_batch_generator, \n",
    "                                  steps_per_epoch=train_batch_generator.__len__(), \n",
    "                                  epochs=100, \n",
    "                                  verbose=1, \n",
    "                                  callbacks=callbacks, \n",
    "                                  validation_data=None, \n",
    "                                  validation_steps=None, \n",
    "                                  class_weight=None, \n",
    "                                  max_queue_size=1, \n",
    "                                  workers=4,\n",
    "                                  use_multiprocessing=False, \n",
    "                                  shuffle=False, \n",
    "                                  initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = PredNetBatchGenerator(batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, z_batch = bg.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 40, 1, 64, 64, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 40, 1, 64, 64, 3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (4, 40, 1, 64, 64, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 40, 3, 64, 64, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 12\n",
    "\n",
    "e_input = K.permute_dimensions(e_input, [1, 0, 2, 3, 4, 5])\n",
    "e_input = K.gather(e_input, [t])\n",
    "e_input = K.permute_dimensions(e_input, [1, 0, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose_3:0' shape=(?, 1, 10, 64, 64, 6) dtype=float32>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_tl: (None, h, w, 3)\n",
    "def X_to_Atl(x_input):\n",
    "    A_tl = K.squeeze(x_input, axis=2) # (None, T, 64, 64, 3)\n",
    "    A_tl = K.permute_dimensions(X, [1, 0, 2, 3, 4, 5]) # (T, None, 64, 64, 3)\n",
    "    A_tl = K.gather(A_tl, [t]) # (1, None, 64, 64, 3)\n",
    "    A_tl = K.squeeze(A_tl, axis=0) # (None, 64, 64, 3)\n",
    "    return A_tl\n",
    "def X_to_Atl_shape(input_shape):\n",
    "    output_shape = (input_shape[0],) + input_shape[-3:]\n",
    "    return output_shape\n",
    "\n",
    "A_tl = keras.layers.Lambda(X_to_Atl, X_to_Atl_shape)(X_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = keras.layers.wrappers.TimeDistributed(\n",
    "    keras.layers.ConvLSTM2D(3, (3, 3), return_sequences=True, return_state=True))(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_loss_weights: (4, 1)\n",
      "time_loss_weights : (10, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prednet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1d1cda913b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprednet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# errors will be (batch_size, nt, nb_layers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m errors_by_time = keras.layers.wrappers.TimeDistributed(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prednet' is not defined"
     ]
    }
   ],
   "source": [
    "nt = 10  # number of timesteps used for sequences in training\n",
    "\n",
    "# Model parameters\n",
    "n_channels, im_height, im_width = (3, 128, 160)\n",
    "input_shape = (n_channels, im_height, im_width) if K.image_data_format() == 'channels_first' else (im_height, im_width, n_channels)\n",
    "\n",
    "stack_sizes = (n_channels, 48, 96, 192)\n",
    "R_stack_sizes = stack_sizes\n",
    "A_filt_sizes = (3, 3, 3)\n",
    "Ahat_filt_sizes = (3, 3, 3, 3)\n",
    "R_filt_sizes = (3, 3, 3, 3)\n",
    "\n",
    "layer_loss_weights = np.array([1., 0., 0., 0.])  # weighting for each layer in final loss; \"L_0\" model:  [1, 0, 0, 0], \"L_all\": [1, 0.1, 0.1, 0.1]\n",
    "layer_loss_weights = np.expand_dims(layer_loss_weights, 1)\n",
    "\n",
    "time_loss_weights = 1./ (nt - 1) * np.ones((nt,1))  # equally weight all timesteps except the first\n",
    "time_loss_weights[0] = 0.\n",
    "\n",
    "\n",
    "print(\"layer_loss_weights:\", layer_loss_weights.shape)\n",
    "print(\"time_loss_weights :\", time_loss_weights.shape)\n",
    "\n",
    "inputs = keras.layers.Input(shape=(nt,) + input_shape)\n",
    "errors = prednet(inputs)  # errors will be (batch_size, nt, nb_layers)\n",
    "\n",
    "errors_by_time = keras.layers.wrappers.TimeDistributed(\n",
    "    keras.layers.Dense(1, trainable=False), \n",
    "    weights=[layer_loss_weights, np.zeros(1)], \n",
    "    trainable=False)(errors)  # calculate weighted error by layer\n",
    "errors_by_time = keras.layers.Flatten()(errors_by_time)  # will be (batch_size, nt)\n",
    "\n",
    "final_errors = keras.layers.Dense(1, weights=[time_loss_weights, np.zeros(1)], trainable=False)(errors_by_time)  # weight errors by time\n",
    "\n",
    "# model\n",
    "model = keras.models.Model(inputs=inputs, outputs=final_errors)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prednet = PredNet(stack_sizes, R_stack_sizes,\n",
    "                  A_filt_sizes, Ahat_filt_sizes, R_filt_sizes,\n",
    "                  output_mode='error', return_sequences=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
